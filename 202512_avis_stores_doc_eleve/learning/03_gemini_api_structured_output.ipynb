{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Analyse IA avec Google Gemini\n",
        "\n",
        "Nous entrons dans le c≈ìur du sujet : l'intelligence artificielle.\n",
        "Nous allons utiliser **Google Gemini** pour analyser nos avis.\n",
        "\n",
        "### Objectifs :\n",
        "1. Comprendre comment appeler l'API Gemini.\n",
        "2. G√©rer les limites de tokens (contexte).\n",
        "3. Passer d'une analyse texte libre √† une donn√©e structur√©e (JSON) exploitable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration de l'API\n",
        "\n",
        "Pour utiliser le mod√®le, il faut une cl√© API.\n",
        "Vous pouvez la r√©cup√©rer gratuitement sur [Google AI Studio](https://aistudio.google.com/).\n",
        "\n",
        "Une fois la cl√© obtenue, le mieux est de la stocker dans un fichier `.env` ou directement dans les secrets de votre environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba22402",
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Chargement des variables d'environnement\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "# Configuration globale de la librairie\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e9e180",
      "metadata": {},
      "source": [
        "## 2. Chargement des Donn√©es\n",
        "\n",
        "Reprenons nos avis consolid√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6406086",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e970ef94",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('df_final.csv')\n",
        "\n",
        "# On garde un √©chantillon repr√©sentatif pour la d√©mo, ou tout le dataset si c'est petit\n",
        "# Pour l'IA, on va concat√©ner le texte des avis pour lui donner du contexte\n",
        "\n",
        "# Prenons les 10 derniers avis pour tester\n",
        "sample_reviews = df.head(10)\n",
        "\n",
        "# Cr√©ation d'un gros bloc de texte avec tous les avis\n",
        "reviews_text = \"\"\n",
        "for index, row in sample_reviews.iterrows():\n",
        "    reviews_text += f\"[Note: {row['rating']}/5] {row['text']}\\n---\\n\"\n",
        "\n",
        "print(f\"Texte pr√©par√© ({len(reviews_text)} caract√®res) :\")\n",
        "print(reviews_text[:500] + \"... [tronqu√©]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32134a84",
      "metadata": {},
      "source": [
        "## 3. Premier Appel Simple (Mod√®le Flash)\n",
        "\n",
        "Nous allons utiliser le mod√®le `gemini-2.5-flash`. C'est le mod√®le le plus rapide et le moins cher, id√©al pour du gros volume.\n",
        "\n",
        "Faisons un test na√Øf : \"Analyse √ßa\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76b447d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_flash = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "prompt_basic = f\"Analyse ces avis :\\n{reviews_text}\"\n",
        "\n",
        "response = model_flash.generate_content(prompt_basic)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2527e38",
      "metadata": {},
      "source": [
        "## 4. La Limite de Tokens\n",
        "\n",
        "Chaque mod√®le a une \"fen√™tre de contexte\". C'est la quantit√© de texte qu'il peut lire en une fois.\n",
        "- **Gemini 2.5 Flash** : 1 Million de tokens (~700 000 mots)\n",
        "- **Gemini 2.5 Pro** : 1 Million de tokens\n",
        "\n",
        "C'est √©norme ! Cela signifie qu'on peut litt√©ralement lui donner des livres entiers.\n",
        "Mais attention, plus on envoie de texte, plus c'est lent et co√ªteux (si version payante)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6654cd62",
      "metadata": {},
      "source": [
        "### V√©rifions la taille de notre prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12519ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# L'API permet de compter les tokens avant d'envoyer\n",
        "token_count = model_flash.count_tokens(prompt_basic)\n",
        "print(f\"Nombre de tokens : {token_count.total_tokens}\")\n",
        "print(f\"Limite du mod√®le : 1 000 000\")\n",
        "\n",
        "if token_count.total_tokens > 1000000:\n",
        "    print(\"‚ö†Ô∏è Attention, trop de texte !\")\n",
        "else:\n",
        "    print(\"‚úÖ √áa passe large !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62572dfe",
      "metadata": {},
      "source": [
        "## 5. Structurer la Sortie (JSON)\n",
        "\n",
        "Le r√©sultat du point 3 √©tait du texte libre. C'est sympa √† lire, mais impossible √† mettre dans un graphique.\n",
        "On va demander au mod√®le Flash de nous r√©pondre en **JSON**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e073e2ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_json = f\"\"\"\n",
        "Analyse ces avis et sors-moi les 3 points forts et les 3 points faibles.\n",
        "R√©ponds uniquement au format JSON comme ceci :\n",
        "{{\n",
        "  \"points_forts\": [\"pf1\", \"pf2\", \"pf3\"],\n",
        "  \"points_faibles\": [\"pf1\", \"pf2\", \"pf3\"]\n",
        "}}\n",
        "\n",
        "AVIS :\n",
        "{reviews_text}\n",
        "\"\"\"\n",
        "\n",
        "response_json = model_flash.generate_content(prompt_json)\n",
        "\n",
        "response_json.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ad4435",
      "metadata": {},
      "source": [
        "## 6. La M√©thode Robuste : Pydantic & Gemini 2.5 Pro\n",
        "\n",
        "Pour une application en production, on ne peut pas juste \"esp√©rer\" que le mod√®le respecte le JSON.\n",
        "On utilise le mode **Structured Output** avec le mod√®le `gemini-2.5-pro` (plus intelligent).\n",
        "On d√©finit la structure exacte voulue avec `Pydantic`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1985e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# 1. On d√©finit l'architecture de la r√©ponse\n",
        "class Theme(BaseModel):\n",
        "    nom: str = Field(description=\"Nom court de la th√©matique (ex: Bug, UX, Prix)\")\n",
        "    sentiment: str = Field(description=\"Positif ou N√©gatif\")\n",
        "    citation: str = Field(description=\"Une citation courte tir√©e des avis qui illustre le th√®me\")\n",
        "\n",
        "class AnalyseResponse(BaseModel):\n",
        "    synthese: str = Field(description=\"R√©sum√© g√©n√©ral en 2 phrases\")\n",
        "    themes: List[Theme] = Field(description=\"Liste des th√®mes principaux identifi√©s\")\n",
        "\n",
        "# 2. On instancie le mod√®le PRO\n",
        "model_pro = genai.GenerativeModel('gemini-2.5-pro')\n",
        "\n",
        "# 3. Appel avec le sch√©ma forc√©\n",
        "prompt_struct = f\"Analyse ces avis en profondeur :\\n{reviews_text}\"\n",
        "\n",
        "response_struct = model_pro.generate_content(\n",
        "    prompt_struct,\n",
        "    generation_config={\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "        \"response_schema\": AnalyseResponse\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response_struct.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1183f4",
      "metadata": {},
      "source": [
        "## 7. Exploitation\n",
        "\n",
        "On peut maintenant charger ce JSON directement en Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc7b4d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Parsing du JSON\n",
        "data = json.loads(response_struct.text)\n",
        "\n",
        "# Affichage propre\n",
        "print(f\"üì¢ SYNTH√àSE : {data['synthese']}\\n\")\n",
        "\n",
        "for theme in data['themes']:\n",
        "    icon = \"‚úÖ\" if theme['sentiment'] == \"Positif\" else \"‚ùå\"\n",
        "    print(f\"{icon} {theme['nom']} : \\\"{theme['citation']}\\\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
